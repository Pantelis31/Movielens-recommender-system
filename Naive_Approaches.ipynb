{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive approaches\n",
    " *Authors : Pantelis Matsakidis & Asteris Barakopoulos*\n",
    " \n",
    " *Group 21*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data and general goal\n",
    "The goal of this assignment is to implement and compare the performance of different recommender system methods. More specifically, the interest lies in estimating the rating that a user would give to a specific movie. The data consists of 1,000,209 anonymus ratings of users that joined MovieLens at the year 2000 and three columns that represent the user id, the movie id and the rating which is on a 5-star scale (whole ratings). There is a total of 6,040 users and 3,952 movies with each user having at least 20 ratings. On this notebook, four approaches are implemented on this data, these are :\n",
    "\n",
    "- $R_{global}(User,Item)=mean(\\text{all ratings})$\n",
    "- $R_{item}(User,Item)=mean(\\text{all ratings for item})$\n",
    "- $R_{user}(User,Item)=mean(\\text{all ratings for user})$\n",
    "- $R_{user-item}(User,Item)= \\alpha*R_{user}(User,Item) + \\beta*R_{item}(User,Item) + \\gamma$\n",
    "\n",
    "Where $R(User,Item)$ stands for the estimate of the rating that an active user would give to a certain item. To assess and compare the performance of these approaches, 5-fold cross-validation is being used in order to get a more honest estimate of their training and test errors. For every approach, the Root mean squared error is being used for assessment and its given by the formula : $$RMSE=\\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} (predicted - true)^2}$$ These are the naive approaches which are characterized that way because of their simplicity. Especially the first three approaches use information from one source only, either the users or the movies (items). So, even though they work up to a certain extend they have some important limitations which are going to be discussed further on the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation\n",
    "### 2.1 $R_{global}(User,Item)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start with the most trivial approach out of the four. This approach estimates the rating that a user would give to a movie according to the mean of every rating that is available in the data. So, no matter for which user or movie is the estimation made for, the result will always be the same mean number of ratings. This is of course very limited in many ways as no differentiation between users or movies takes place. Basically, the recommendations that a user gets according to this approach are as good as random since the estimated rating is the same for any movie. The results obtained from the 5-fold cross validation are presented on the following table :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Training error | Test error   |\n",
    "|------|------|\n",
    "|   1,117  | 1,117|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###Modules\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###Read data\n",
    "ratings = np.genfromtxt(\"ratings.dat\",names = None,dtype = None,delimiter = \"::\")\n",
    "\n",
    "\n",
    "\n",
    "###-----------------------------------Rglobal - mean of all ratings---------------------------------------------------###\n",
    "nfolds = 5\n",
    "train_error = np.zeros(nfolds)\n",
    "test_error = np.zeros(nfolds)\n",
    "np.random.seed(17)\n",
    "seqs=[x%nfolds for x in range(len(ratings))]\n",
    "np.random.shuffle(seqs)\n",
    "\n",
    "for fold in range(nfolds):\n",
    "    train_sel=np.array([x!=fold for x in seqs])\n",
    "    test_sel=np.array([x==fold for x in seqs])\n",
    "    train=ratings[train_sel]\n",
    "    test=ratings[test_sel]\n",
    "    #calculate model parameters: mean rating over the training set:\n",
    "    Rglobal=np.mean(train[:,2])\n",
    "    #apply the model to the train set:\n",
    "    train_error[fold]=np.sqrt(np.mean((train[:,2]-Rglobal)**2))\n",
    "    #apply the model to the test set:\n",
    "    test_error[fold]=np.sqrt(np.mean((test[:,2]-Rglobal)**2)) \n",
    "    #print errors:\n",
    "    print(\"Fold \" + str(fold) + \": RMSE_train=\" + str(train_error[fold]) + \"; RMSE_test=\" + str(test_error[fold]))\n",
    "\n",
    "#print the final conclusion:\n",
    "print(\"\\n\")\n",
    "print(\"Mean error on TRAIN: \" + str(np.mean(train_error)))\n",
    "print(\"Mean error on  TEST: \" + str(np.mean(test_error)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 $R_{item}(User,Item)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is certainly much better than the previous one, but still with some serious limitations. Here, we focus on the movie ratings. The estimated rating that a user would give to a movie is equal to the mean of all ratings for this specific movie. Even though it perfoms better than the global mean rating, it eventually recommends movies that are highly rated by most people. This may seem reasonable at first, but the problem is that it doen't take into account the user's personal taste but only what most of the people find good. Its performance also depends on the data. For example, if a movie has been only highly/poorly rated by a small number of users, then the recommendations that other users will get for this movie will not be very honest. Its recommendations can only be trusted for movies that have been rated many times and by a variety of users. During cross-validation, there were some cases where a movie id was present in the test set but not in the training set. For those movies, the estimated rating was done according to the previous approach using the $R_{global}(User,Item)$. Finally, the error on the training and test set for this approach can be seen from the table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Training error | Test error   |\n",
    "|------|------|\n",
    "|   0.974  | 0.979|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------------Ritem - mean of all ratings for a specific item----------------------------------------------------------------###\n",
    "train_error2 = np.zeros(nfolds)\n",
    "test_error2 = np.zeros(nfolds)\n",
    "np.random.seed(17)\n",
    "seqs=[x%nfolds for x in range(len(ratings))]\n",
    "np.random.shuffle(seqs)\n",
    "\n",
    "for fold in range(nfolds):\n",
    "    train_sel=np.array([x!=fold for x in seqs])\n",
    "    test_sel=np.array([x==fold for x in seqs])\n",
    "    train=ratings[train_sel]\n",
    "    test=ratings[test_sel]\n",
    "    Rglobal=np.mean(train[:,2])\n",
    "    #Make a list which has the mean rating for every movie in the training set\n",
    "    Ritem = list()\n",
    "    for i in range(1,3953):\n",
    "        if i in train[:,1]:\n",
    "            my_table = train[train[:,1] == i]\n",
    "            Ritem.append(my_table[:,2].mean())\n",
    "        else:\n",
    "            #If a movie id is not in the training set we replace its estimate with Rglobal\n",
    "            Ritem.append(Rglobal)\n",
    "\n",
    "    Ritem_train = list()\n",
    "    for i in range(train.shape[0]):\n",
    "        Ritem_train.append(Ritem[int(train[i,1])-1])\n",
    "\n",
    "    Ritem_test = list()\n",
    "    for i in range(test.shape[0]):\n",
    "        Ritem_test.append(Ritem[int(test[i,1])-1])\n",
    "\n",
    "    Ritem_train = np.array(Ritem_train)\n",
    "    Ritem_test = np.array(Ritem_test)\n",
    "    #apply the model to the train set:\n",
    "    train_error2[fold]=np.sqrt(np.mean((train[:,2]-Ritem_train)**2))\n",
    "    #apply the model to the test set:\n",
    "    test_error2[fold]=np.sqrt(np.mean((test[:,2]-Ritem_test)**2)) \n",
    "    #print errors:\n",
    "    print(\"Fold \" + str(fold) + \": RMSE_train=\" + str(train_error2[fold]) + \"; RMSE_test=\" + str(test_error2[fold]))\n",
    "    # Saving the Ritem vectors for the training and test sets at each fold,\n",
    "    # to use them later at the linear regression approach.\n",
    "    if fold == 0:\n",
    "        Ritrain0 = Ritem_train\n",
    "        Ritest0 = Ritem_test\n",
    "    elif fold == 1:\n",
    "        Ritrain1 = Ritem_train\n",
    "        Ritest1 = Ritem_test\n",
    "    elif fold == 2:\n",
    "        Ritrain2 = Ritem_train\n",
    "        Ritest2 = Ritem_test\n",
    "    elif fold == 3:\n",
    "        Ritrain3 = Ritem_train\n",
    "        Ritest3 = Ritem_test\n",
    "    elif fold == 4:\n",
    "        Ritrain4 = Ritem_train\n",
    "        Ritest4 = Ritem_test\n",
    "\n",
    "#print the final conclusion:\n",
    "print(\"\\n\")\n",
    "print(\"Mean error on TRAIN: \" + str(np.mean(train_error2)))\n",
    "print(\"Mean error on  TEST: \" + str(np.mean(test_error2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 $R_{user}(User,Item)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, the estimated rating that a user would give to a movie is equal to the mean of all ratings of the specific user. Initially, one may think that this approach would perform better than the previous one but its actually slightly worse. This is mainly because the general qualiy of a movie, or its general rating is not taken into account at all. Therefore, if for example a user has a high mean rating, then any movie recommended to this user is expected to have a high rating regardless of its genre or its general rating. Like the previous approach, for the user ids that were present in the test set but not in the training set, their estimated mean ratings were found using $R_{global}(User,Item)$. The results can be seen below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Training error | Test error   |\n",
    "|------|------|\n",
    "|   1.027  | 1.035|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------------Ruser - mean of all ratings for specific user-----------------------------###\n",
    "train_error3 = np.zeros(nfolds)\n",
    "test_error3 = np.zeros(nfolds)\n",
    "np.random.seed(17)\n",
    "seqs=[x%nfolds for x in range(len(ratings))]\n",
    "np.random.shuffle(seqs)\n",
    "\n",
    "for fold in range(nfolds):\n",
    "    train_sel=np.array([x!=fold for x in seqs])\n",
    "    test_sel=np.array([x==fold for x in seqs])\n",
    "    train=ratings[train_sel]\n",
    "    test=ratings[test_sel]\n",
    "    Rglobal=np.mean(train[:,2])\n",
    "    #Make a list which has the mean rating for every user in the training set\n",
    "    Ruser = list()\n",
    "    for i in range(1,6041):\n",
    "        if i in train[:,0]:\n",
    "            my_table = train[train[:,0] == i]\n",
    "            Ruser.append(my_table[:,2].mean()) \n",
    "        else:\n",
    "            #If a user id is not in the training set we replace its estimate with Rglobal\n",
    "            Ruser.append(Rglobal)\n",
    "\n",
    "    Ruser_train = list()\n",
    "    for i in range(train.shape[0]):\n",
    "        Ruser_train.append(Ruser[int(train[i,0])-1])\n",
    "\n",
    "    \n",
    "\n",
    "    Ruser_test = list()\n",
    "    for i in range(test.shape[0]):\n",
    "        Ruser_test.append(Ruser[int(test[i,0])-1])\n",
    "\n",
    "    \n",
    "\n",
    "    Ruser_train = np.array(Ruser_train)\n",
    "    Ruser_test = np.array(Ruser_test)\n",
    "    \n",
    "    \n",
    "    #apply the model to the train set:\n",
    "    train_error3[fold]=np.sqrt(np.mean((train[:,2]-Ruser_train)**2))\n",
    "    #apply the model to the test set:\n",
    "    test_error3[fold]=np.sqrt(np.mean((test[:,2]-Ruser_test)**2)) \n",
    "    #print errors:\n",
    "    print(\"Fold \" + str(fold) + \": RMSE_train=\" + str(train_error3[fold]) + \"; RMSE_test=\" + str(test_error3[fold]))\n",
    "    if fold == 0:\n",
    "        Rutrain0 = Ruser_train\n",
    "        Rutest0 = Ruser_test\n",
    "    elif fold == 1:\n",
    "        Rutrain1 = Ruser_train\n",
    "        Rutest1 = Ruser_test\n",
    "    elif fold == 2:\n",
    "        Rutrain2 = Ruser_train\n",
    "        Rutest2 = Ruser_test\n",
    "    elif fold == 3:\n",
    "        Rutrain3 = Ruser_train\n",
    "        Rutest3 = Ruser_test\n",
    "    elif fold == 4:\n",
    "        Rutrain4 = Ruser_train\n",
    "        Rutest4 = Ruser_test\n",
    "\n",
    "\n",
    "#print the final conclusion:\n",
    "print(\"\\n\")\n",
    "print(\"Mean error on TRAIN: \" + str(np.mean(train_error3)))\n",
    "print(\"Mean error on  TEST: \" + str(np.mean(test_error3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 $R_{user-item}(User,Item)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final approach in this part, a linear model is being fitted using two variables : $R_{user}(User,Item)$ and $R_{item}(User,Item)$ that were found in the above sections. It is expected that this model will perform much better than each of the previous methods individually. That is because the response variable $R_{user-item}(User,Item)$, takes into account information for both users and movies. The parameters were estimated using ordinary least squares estimation, giving the following linear equation : $R_{user-item}(User,Item)= 0.781*R_{user}(User,Item) + 0.874*R_{item}(User,Item) - 2.348$. From the table below, one can see the performance of all the four naive approaches that have been implemented so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                   | $R_{global}$| $R_{user}$| $R_{item}$| $R_{user-item}$|\n",
    "|-------------------|-------------|-----------|-----------|----------------|\n",
    "|   Training error  | 1.117       | 1.027     | 0.974     | 0.914          |\n",
    "|                   |             |           |           |                |\n",
    "|   Test error      | 1.117       | 1.035     | 0.979     | 0.924          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error4 = np.zeros(nfolds)\n",
    "test_error4 = np.zeros(nfolds)\n",
    "np.random.seed(17)\n",
    "seqs=[x%nfolds for x in range(len(ratings))]\n",
    "np.random.shuffle(seqs)\n",
    "\n",
    "for fold in range(nfolds):\n",
    "    train_sel=np.array([x!=fold for x in seqs])\n",
    "    test_sel=np.array([x==fold for x in seqs])\n",
    "    train=ratings[train_sel]\n",
    "    test=ratings[test_sel]\n",
    "    #Calculate Ritem for the training set.\n",
    "    #mean rating for each movie on the training set\n",
    "    Rglobal=np.mean(train[:,2])\n",
    "    if fold == 0 :\n",
    "        X1 = np.vstack([Rutrain0,Ritrain0])\n",
    "        X1 = X1.T\n",
    "        Y1 = train[:,2]\n",
    "        Y1 = Y1.T\n",
    "        A1 = np.hstack([X1,np.ones((X1.shape[0],1))])\n",
    "        model_parameters, residuals, rank, singular_values = np.linalg.lstsq(A1, Y1,rcond=None)\n",
    "        model_parameters = np.array(model_parameters)\n",
    "        model_parameters.shape = (1,3)\n",
    "        X2 = np.vstack([Rutest0,Ritest0])\n",
    "        X2 = X2.T\n",
    "        Y2 = test[:,2]\n",
    "        Y2 = Y2.T\n",
    "        A2 = np.hstack([X2,np.ones((X2.shape[0],1))])\n",
    "    elif fold == 1 :\n",
    "        X1 = np.vstack([Rutrain1,Ritrain1])\n",
    "        X1 = X1.T\n",
    "        Y1 = train[:,2]\n",
    "        Y1 = Y1.T\n",
    "        A1 = np.hstack([X1,np.ones((X1.shape[0],1))])\n",
    "        model_parameters, residuals, rank, singular_values = np.linalg.lstsq(A1, Y1,rcond=None)\n",
    "        model_parameters = np.array(model_parameters)\n",
    "        model_parameters.shape = (1,3)\n",
    "        X2 = np.vstack([Rutest1,Ritest1])\n",
    "        X2 = X2.T\n",
    "        Y2 = test[:,2]\n",
    "        Y2 = Y2.T\n",
    "        A2 = np.hstack([X2,np.ones((X2.shape[0],1))])\n",
    "    elif fold == 2 :\n",
    "        X1 = np.vstack([Rutrain2,Ritrain2])\n",
    "        X1 = X1.T\n",
    "        Y1 = train[:,2]\n",
    "        Y1 = Y1.T\n",
    "        A1 = np.hstack([X1,np.ones((X1.shape[0],1))])\n",
    "        model_parameters, residuals, rank, singular_values = np.linalg.lstsq(A1, Y1,rcond=None)\n",
    "        model_parameters = np.array(model_parameters)\n",
    "        model_parameters.shape = (1,3)\n",
    "        X2 = np.vstack([Rutest2,Ritest2])\n",
    "        X2 = X2.T\n",
    "        Y2 = test[:,2]\n",
    "        Y2 = Y2.T\n",
    "        A2 = np.hstack([X2,np.ones((X2.shape[0],1))])\n",
    "    elif fold == 3 :\n",
    "        X1 = np.vstack([Rutrain3,Ritrain3])\n",
    "        X1 = X1.T\n",
    "        Y1 = train[:,2]\n",
    "        Y1 = Y1.T\n",
    "        A1 = np.hstack([X1,np.ones((X1.shape[0],1))])\n",
    "        model_parameters, residuals, rank, singular_values = np.linalg.lstsq(A1, Y1,rcond=None)\n",
    "        model_parameters = np.array(model_parameters)\n",
    "        model_parameters.shape = (1,3)\n",
    "        X2 = np.vstack([Rutest3,Ritest3])\n",
    "        X2 = X2.T\n",
    "        Y2 = test[:,2]\n",
    "        Y2 = Y2.T\n",
    "        A2 = np.hstack([X2,np.ones((X2.shape[0],1))])\n",
    "    elif fold == 4 :\n",
    "        X1 = np.vstack([Rutrain4,Ritrain4])\n",
    "        X1 = X1.T\n",
    "        Y1 = train[:,2]\n",
    "        Y1 = Y1.T\n",
    "        A1 = np.hstack([X1,np.ones((X1.shape[0],1))])\n",
    "        model_parameters, residuals, rank, singular_values = np.linalg.lstsq(A1, Y1,rcond=None)\n",
    "        model_parameters = np.array(model_parameters)\n",
    "        model_parameters.shape = (1,3)\n",
    "        X2 = np.vstack([Rutest4,Ritest4])\n",
    "        X2 = X2.T\n",
    "        Y2 = test[:,2]\n",
    "        Y2 = Y2.T\n",
    "        A2 = np.hstack([X2,np.ones((X2.shape[0],1))])\n",
    "    #apply the model to the train set:\n",
    "    train_error4[fold]=np.sqrt(((np.dot(model_parameters,A1.T) - Y1) ** 2).mean())\n",
    "    #apply the model to the test set:\n",
    "    test_error4[fold]=np.sqrt(((np.dot(model_parameters,A2.T) - Y2) ** 2).mean())\n",
    "    #print errors:\n",
    "    print(\"Fold \" + str(fold) + \": RMSE_train=\" + str(train_error4[fold]) + \"; RMSE_test=\" + str(test_error4[fold]))\n",
    "\n",
    "\n",
    "print(\"Mean error on TRAIN: \" + str(np.mean(train_error4)))\n",
    "print(\"Mean error on  TEST: \" + str(np.mean(test_error4)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
